--- 
layout: post
title: 若往若还-决策树 (下)
category: blog
description:  决策树算法也是十大经典算法之一，基本每一本机器学习的书都会介绍决策树算法。1966年Hunt提出了CLS算法是最早的决策树算法。1986年Quinlan提出的ID3和1993年提出的C4.5是最具影响力的决策树算法。而今决策树算法已经成为了机器学习中最为经典的算法之一，受到了机器学习研究者的广泛关注。
---

## 背景知识
  　　本篇继续上一篇文章介绍决策树的相关知识。


## 关键步骤

### 决策树生成
　　决策树生成最为常用的是ID3算法和C4.5算法，下面依次介绍。

#### ID3算法
　　从根结点开始，对所有特征计算信息增益，取信息增益最大的特征作为结点的特征，然后根据该特征的不同取值生成子结点，再对子结点递归地调用上述方法，直至所有特征的信息增益都小于某阈值或没有特征可以选择，最终生成决策树。ID3算法相当于用极大似然估计生成模型。

#### C4.5算法
　　C4.5算法和ID3算法类似，唯一不同的地方是在生成决策树过程中采用信息增益比选择特征，而非信息增益。决策树生成步骤同ID3算法。

### 决策树剪枝
　　通过上述算法生成的决策树，可以对训练数据集进行比较好的拟合，但是泛化能力较差，需要通过剪枝避免过拟合、提高泛化能力。决策树剪枝的具体做法是，裁减掉决策树的某些子树或者叶子结点，使其子树的根结点或者叶子结点的父结点作为新的叶子结点，简化决策树模型。应该裁剪哪些子树或叶子结点，通过极小化整体的损失函数来选择。  
　　假设树T的叶子结点数为|T|，t是树T的叶子结点，叶子结点t有\\(N_{t}\\)个样本，其中k类的样本有\\(N_{tk}\\)个，k=1,2,...,K，\\(H_{t}(T)\\)为叶子结点t上的经验熵，\\(\alpha\ge 0\\)为参数，则损失函数可以定义为：
\\[C_{\alpha}(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)+\alpha|T|\\]
其中经验熵为
\\[H_{t}(T)=-\sum_{k}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}\\]
　　在损失函数中，将右端第一项记为
\\[C(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}log\frac{N_{tk}}{N_{t}}\\]
这时有
\\[C_{\alpha}(T)=C(T)+\alpha|T|\\]
　　其中C(T)表示模型对于训练数据集的预测误差，叶子结点数|T|表示模型的复杂度。参数\\(\alpha\\)平衡C(T)和|T|两者的影响，取值较大时促使选择更简单的模型，取值较小时促使选择更复杂的模型，取值为0时表示不考虑模型复杂度。  
　　在进行剪枝时，一般子树越大对训练集拟合程度越好，即C(T)越小，但是模型越复杂，即|T|越大。子树越小则拟合程度越差，即C(T)越大，模型越简单，即|T|越小。决策树的损失函数是考虑了这两者的平衡。决策树的生成是通过提高信息增益（信息增益比）来对训练集进行更好的拟合，而剪枝则不但考虑到整体的经验熵还考虑了模型的复杂度，在这两方面对模型进行剪枝优化。  
* 树的剪枝算法：
输入：整个决策树T，参数\\(\alpha\\)
输出：剪枝后的树T
（1）计算每个结点的经验熵
（2）递归的对叶子结点进行回缩
假设回缩前和回缩后的树对应的损失函数分别为\\(C_{\alpha}(T_{A})\\)和\\(C_{\alpha}(T_{B})\\)，如果
\\[C_{\alpha}(T_{A})\le C_{\alpha}(T_{B})\\]
则进行剪枝，其父结点变为新的叶子结点。
重复步骤（2）直至损失函数最小，得到剪枝后的子树\\(T_{\alpha}\\)。

## CART算法
　　CART(classification and regression tree)算法由Breiman在1984年提出，是决策树中比较经典的算法，在实际应用中得到了普遍的推广。CART算法对于以后学习GBDT或者XGBoost等具有很好的促进作用，因此我们单独用一节的篇幅来介绍CART算法，希望大家能熟练掌握。CART的决策树是二叉树，左分支取值为‘是’，右分支取值为‘否’。这样构建CART树等价于将每个特征按某一取值进行二分，将特征空间划分为有限的几个单元，然后对这些单元预测输入给定条件下输出的条件概率分布。CART算法主要分为两步，决策树生成和决策树剪枝。

### 决策树生成  
#### 回归树生成

　　X和Y分别为输入、输出变量，Y是连续变量，训练集为：
\\[D={(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{N},y_{N})}\\]
我们可以将输入空间划分为M个单元，\\(R_{1},R_{2},...,R_{M}\\),\\(R_{M}\\)对应输出值\\(c_{M}\\),则回归树模型的数学表示为：
\\[f(x)=\sum_{m=1}^{M}c_{m}I(x \in R_{m})\\]
　　我们用平方差\\(\sum_{x_{i}\in R_{m}}(y_{i}-f(x_{i}))^{2}\\)来表示实际值与预测值之间的误差，选择平方差最小的预测值作为每个单元上的最优输出值\\(c_{m}\\),可知，当输出值为实际值y的均值时为最优
\\[\hat{c_{m}}=avg(y_{i}|x_{i} \in R_{m})\\]
　　这样就找到了每个叶子结点对应的最优输出值。那么问题来了，怎么对输入空间进行划分呢？因为这是一个NP问题，所以我们采用启发式算法。选择第j个特征，和它的取值s,作为切分变量和切分点，定义两个区域：
\\[R_{1}(j,s)={x|x^{(j)}\le s} 和 R_{2}(j,s)={x|x^{(j)} > s}\\]
然后，我们需要找到最优的切分变量和最优的切分点。对于输入变量j，找到最优的切分点，求解如下：
\\[min_{j,s}[min_{c_{1}}\sum_{x_{i}\in R_{1}(j,s)}(y_{i}-c_{1})^{2}+min_{c_{2}}\sum_{x_{i}\in R_{2}(j,s)}(y_{i}-c_{2})^{2}]\\]
　　以每个特征作为输入变量，找到最优的切分变量和其最优切分点。以此切分变量和切分点，将输入空间划分为两部分，再重复上述过程，直至满足停止条件，生成一棵完整的回归树，这样的回归树称之为最小二乘回归树。

#### 分类树生成

　　分类树与回归树有所不同，分类树是通过基尼系数来选择最优特征并决定最优切分点。
基尼系数定义：
　　分类问题中，设有K个类，样本属于第k类的概率是\\(p_{k}\\)，则基尼系数定义为：
\\[Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}\\]
所以对于样本集合D，其基尼系数为：
\\[Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_{k}|}{|D|})^{2}\\]
如果样本集合D被特征A分为两部分，分别为\\(D_{1}、D_{2}\\),则在特征A的条件下，D的基尼系数为：
\\[Gini(D,A)=\frac{|D_{1}|}{|D|}Gini(D_{1})+{|D_{2}|}{|D|}Gini(D_{2})\\]

CART生成算法
输入：训练集D，停止计算条件
输出：CART决策树
（1）训练集为D，计算所有特征的基尼系数
（2）在所有特征及其切分点中，选择基尼系数最小的特征及其切分点作为最优特征和最优切分点。按照最优特征和最优切分点，将结点分裂出两个子结点
（3）递归(1)(2)两步
（4）直至满足停止计算条件，生成CART决策树

### 决策树剪枝 

　　CART树生成之后，为了提高其泛化能力，我们需要对其剪枝，本节主要介绍CART树剪枝的算法。本节内容理解起来相对较难，我尽可能用通俗的语言表达清楚。

　　CART剪枝主要分为两步，第一步是从决策树的底部开始剪枝，一直剪到根结点，形成一个子树的序列\\({T_{1},T_{2},...,T_{N}}\\)。第二步是通过交叉验证的方法验证这个子树的序列，得到最优子树。

#### 剪枝
　　剪枝过程中，子树的损失函数
\\[C_{\alpha}(T)=C(T)+\alpha|T|\\]
　　其中C(T)用来评估模型对于训练数据集的误差，|T|表示模型复杂度，\\(\alpha\\)用来平衡训练集误差和模型复杂度。
　　假设\\(\alpha\\)固定，则可知，一定存在一棵子树使\\(C_{\alpha}(T)\\)最小，这棵子树即为最优子树。由于一棵确定的树的子树序列是确定的，因此，对于整体决策树中任意内部结点t,以t为单位的损失函数是
\\[C_{\alpha}(t)=C(t)+\alpha|T|\\]
以t为根结点的子树\\(T_{t}\\)的损失函数是：
\\[C_{\alpha}(T_{t})=C(T_{t})+\alpha|T|\\]
当\\(\alpha\\)=0或者比较小时
\\[C_{\alpha}(T_{t})<C_{\alpha}(t)\\]
当\\(\alpha\\)达到某一值时
\\[C_{\alpha}(T_{t})=C_{\alpha}(t)\\]
\\(\alpha\\)再增大时，不等式反向。所以只要\\(\alpha \ge \frac{C(t)-C(T_{t})}{|T_{t}|-1}\\)时，剪枝必然会发生（此时是假设某一子树的情况下）。对于某一确定的子树，损失函数中其他变量都是已知的，只有\\(\alpha\\)是可变的，且\\(\alpha \ge \frac{C(t)-C(T_{t})}{|T_{t}|-1}\\)，因此若使得\\(C_{\alpha}(T_{t})\\)最小，则\\( \alpha=\frac{C(t)-C(T_{t})}{|T_{t}|-1} \\) 。    

#### 交叉验证最优子树

　　对于确定的训练集，可以确定其子树序列，通过子树序列确定最优的\\(\alpha\\)，则可以求得每个子树的\\(C_{\alpha}(T)\\)，采用交叉验证的方法，选出损失函数最小的子树，即为最优子树。


## 写在最后
　　由于决策树算法涉及知识较多，因此分为两篇文章为大家介绍。如果大家有什么疑问，可以回复公共号留言，我将尽我所能为大家解答，另外也可以关注我的微信公众号：机器学习让数据说话。下一期将为大家介绍XGBoost实现原理，敬请关注！


## 参考资料
　　《统计机器学习》	李航著
