--- 
layout: post
title: 【原创】跳舞的大象-KNN算法 
category: blog
description: 国际权威学术会议ICDM在2006年曾评选出数据挖掘十大经典算法，KNN（K-Nearest-Neighbor-Classification）便是其中之一。KNN算法是一个理论上比较成熟的算法 
---

## 算法简介
  　　国际权威学术会议ICDM曾在2006年评选出数据挖掘十大经典算法，KNN（K-Nearest-Neighbor-Classification）便是其中之一。KNN算法是一个理论上比较成熟的算法，在机器学习中也是一个较为简单的算法，它的核心思想用一句通俗的话来讲就是“近朱者赤，近墨者黑”，同一类型的事物通常会更相近。其主要算法思想为：特征空间中的一个样本，如果与其最相似的k个样本中大部分属于某个类别，则该样本也属于该类别。行业中KNN算法主要应用于客户流失预测、文本分类等。


## 数学推导

　　在李航教授的《统计机器学习》一书中，对KNN的数学表示如下，此处直接引用李老师的数学表示：

　　输入：训练数据集 
\\[T=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{N},y_{N})\}\\]
其中，\\(x_{i}\in\chi\subseteq R^n\\) 为实例的特征向量， \\(y_{i}\in Y = {c_{1}, c{2},...,c_{K} }\\)为实例类别，i=1,2,...N; 实例特征向量x;


　　输出：实例x所属的类y

（1）根据给定的距离度量，在训练集T中找到与x最邻近的k个点，涵盖k个点的x的邻域记作\\(N_{k}(x)\\)

（2）在\\(N_{k}(x)\\)中根据分类决策规则（如多数表决）决定x的类别y：

\\[y = \arg\max_{c_{j}} \sum_{x_{i}\in N_{k}(x)} I(y_{i} = c_{j}) , i = 1,2,...,N; j = 1,2,...,K \\]

上式中，I为指示函数，即当\\(y_{i} = c_{j}\\)时，I为1，否则I为0。

　　K近邻法的特殊情况是k=1的情形，称为最近邻算法，对于输入的实例点x，最近邻法将训练数据集中与x最邻近点的类别作为x的类别。



## 关键因素

### 距离度量

　　KNN算法的第一个关键因素是距离度量，我们采用什么样的距离度量对于模型的准确性是至关重要的。常用的距离度量有欧式距离、余弦相似度等。

* 欧式距离：多维空间中各点之间的绝对距离。
\\[dist(X,Y) = \sqrt {\sum_{i = 1}^{n} (x_{i} - y_{i})^{2}}\\]
* 明科夫斯基距离：也称明式距离，欧式距离的一般形式，当p=2时即为欧式距离。
\\[dist(X,Y) = (\sum_{i = 1}^{n} |x_{i} - y_{i}|^{p})^{1/p}\\]

* 曼哈顿距离：当明式距离中的p=1时即为曼哈顿距离。
\\[dist(X,Y) = \sum_{i = 1}^{n} |x_{i} - y_{i}|\\]
* 余弦相似度：在向量空间中，通过两个向量的余弦值来衡量其相似度的大小。
\\[sim(X,Y) = \cos\theta = \frac{\vec{x}\cdot\vec{y}}{\Vert{x}\Vert\cdot\Vert{y}\Vert}\\]

### K值选择
　　K值的选择也是影响KNN模型预测结果的重要因素。选择的K值较小的话，会用一个比较小的邻域中的训练样本进行比较，降低了近似误差，即只有和训练样本特别相似的情况下才会对预测结果起作用，但是较小的K值会增加估计误差，即预测结果对于非常近似的点非常敏感，如果这些点是一些噪声点，则可能会造成预测结果不准确。选择的K值较大的话则情况相反，即会降低估计误差，增加近似误差。K值一般通过交叉检验来确定。

### 决策规则
　　决策规则是KNN算法中不可或缺的一部分，目前KNN算法的决策规则多数采用多数表决，即邻域中的训练样本中，多数样本属于某一类别，则预测样本属于该类别。

### 归一化
　　如果在KNN 中某一特征值域非常大，那么在做距离计算时该特征可能占据很大比重，未必符合实际情况，此时需要对特征数据进行归一化。


## 优缺点

优点：
* 简单有效，易于实现
* 无需训练

缺点：
* 懒惰算法，数据集的样本容量大时计算量比较大，评分慢
* 样本不平衡时，预测偏差比较大

## 代码实现

 　　下面是KNN算法的一个简单实现，供大家参考，大家可以自己实现一下KNN算法，加深理解。

	import numpy as np

	def knn(x, dataSet, labels, k):
	    dataSize = dataSet.shape[0]
	    dataSub = np.tile(x, (dataSize,1)) - dataSet
	    dataDis = ((dataSub**2).sum(axis = 1))**0.5
	    sortIndex = dataDis.argsort()
	    classCount = {}
	    for i in range(k):
	        label = labels[sortIndex[i]]
	        classCount[label] = classCount.get(label, 0) + 1
	    result = sorted(classCount.iteritems(), key = lambda x: x[1], reverse = True)[0][0]
	    return result


	x = (1,4)
	dataSet = np.array([(2,2),(1,3),(2,7),(3,3),(1,4)])
	labels = np.array([1,1,0,0,2])
	k = 1
	result = knn(x, dataSet, labels, k)
	print (result)

## 写在最后
　　OK!KNN算法就介绍到这里，如果大家有什么疑问，可以回复公共号留言，我将尽我所能为大家进行解答，另外也可以关注我的微信公众号：机器学习让数据说话。下一期将为大家介绍决策树算法，敬请关注！

