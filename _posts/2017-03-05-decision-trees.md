--- 
layout: post
title: 若往若还-决策树 (上)
category: blog
description:  决策树算法也是十大经典算法之一，基本每一本机器学习的书都会介绍决策树算法。1966年Hunt提出了CLS算法是最早的决策树算法。1986年Quinlan提出的ID3和1993年提出的C4.5是最具影响力的决策树算法。而今决策树算法已经成为了机器学习中最为经典的算法之一，受到了机器学习研究者的广泛关注。
---

## 背景知识
  　　决策树算法也是十大经典算法之一，基本每一本机器学习的书都会介绍决策树算法。1966年Hunt提出了CLS算法是最早的决策树算法。1986年Quinlan提出的ID3和1993年提出的C4.5是最具影响力的决策树算法。而今决策树算法已经成为了机器学习中最为经典的算法之一，受到了机器学习研究者的广泛关注。


## 算法介绍
### 决策树定义
　　决策树定义采用李航老师在《统计学习方法》的定义：  
　　分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点（node）和有向边（directed edge）组成，结点有两种类型：内部结点（internal node）和叶节点（leaf node），内部结点表示一个特征或属性，叶节点表示一个类。  
　　用决策树进行分类，类似于程序中的if-else结构，从根结点开始，对第一个特征进行分类，根据其特征的测试结果分配到其子结点。比如年龄特征是一个结点，年龄大于30岁的为分到其左子结点，否则分到右结点。决策树路径具备一个重要的性质：互斥且完备。也就是说每一个样本最终肯定会被分到某一个类中（肯定会被分到某类且唯一确定的一个类）  
　　决策树还表示给定特征下的条件概率分布。即对特征空间进行划分，且这些划分互不相交，并在每一个划分定义一个类的概率分布，从而构成了条件概率分布。假设X为特征的随机变量，Y为类的随机变量，则此条件概率表示为P(Y|X)。  

### 决策树学习
　　决策树的训练目标就是训练出一种分类规则使得数据集中所有的样本都能分到其正确的分类。但这样的决策树可能有多个，也可能没有，所以我们的目标是找到一颗能将大部分样本正确分类并且具有较好的泛化能力。决策树学习的损失函数一般是正则化的极大似然函数。因为从所有决策树下选择最优的决策树是NP完全问题，因此一般会采用启发式算法近似求解。  
　　首先，先从根节点选择一个最优的特征对数据集进行划分，然后依次再对其子结点进行最优的划分，即每一步求其局部最优解，直至该子集所有样本都被正确的分类，则生成分类对应的叶子节点。如果子集还有没有被正确分类的样本则继续分裂，直至生成一颗完整的决策树。  
　　这样生成的决策树虽然可以对现有样本进行很好的分类，但是泛化能力不一定好，因此需要进行剪枝。另外，如果特征很多则需要进行特征选择，下一节将详细进行介绍。  


## 关键步骤

### 特征选择

　　特征选择即选择出哪些有分类能力的特征。对于那些分类结果和随机分类结果无明显差别的特征，我们会将其丢弃掉，这样可以提高决策树学习的效率。那怎么衡量特征是否有分类能力呢？  
* 熵和条件熵
　　熵（entropy）是表示随机变量不确定性的度量，假设X是一个取值有限的随机变量，其概率分布为  
	\\[P(X=x_{i})=p_{i}, i=1,2,...,n\\]
则随机变量X的熵定义为  
\\[H(p)=-\sum_{i=1}^{n}p_{i}logp_{i}\\]
若\\(p_{i}=0\\)，则定义0log0=0。式中的对数以2为底或以e为底，熵的单位分别为比特或纳特。熵越大，随机变量的不确定性就越大  

设有随机变量（X,Y），其联合概率分布为：  
\\[P(X=x_{i},Y=y_{i})=p_{ij}, i=1,2,...,n; j=1,2,...,m\\]
条件熵H[Y|X]表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定条件下随机变量Y的条件熵（conditional entropy）H(Y|X),定义为X给定条件下Y的条件概率分布的熵对X的数学期望  
\\[H(Y|X)=\sum_{i=1}^{n}p_{i}H(Y|X=x_{i})\\]

这里，\\(p_{i}=P(X=x_{i}), i=1,2,...,n\\)  
　　当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。

#### 信息增益

　　信息增益（information gain）表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。  
定义：  
　　　特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即  
\\[g(D,A)=H(D)-H(D|A)\\]
　　一般地，熵H(Y)与条件熵H（Y|X）之差称为互信息（mutual information），决策树学习中的信息增益等价于训练数据集中类与特征的互信息。  
　　决策树的学习即通过信息增益来选择特征，给定训练数据集D和特征A，经验熵H(D)表示数据集D进行分类的不确定性。而经验条件熵H(D|A)表示在特征A给定的条件下D进行分类的不确定性。他们的差即表示特征A使得数据集D分类不确定性的降低程度。所以对于决策树的特征选择来讲，首先计算所有特征的信息增益，然后选择信息增益最大的特征。  

信息增益算法：  
　　输入：训练数据集D和特征A  
　　输出：特征A对训练数据集D的信息增益g(D,A)  
(1)计算数据集D的经验熵H(D)
\\[H(D)=-\sum_{k=1}^{K}\frac{|C_{k}|}{D}log_{2}\frac{|C_{k}|}{D}\\]
(2)计算特征A对数据集D的经验条件熵H(D|A)  
\\[H(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{D}H(D_{i})=-\sum_{i=1}^{n}\frac{|D_{i}|}{D}\sum_{k=1}^{K}\frac{D_{ik}}{D_{i}}log_{2}\frac{D_{ik}}{D_{i}}\\]
(3)计算信息增益  
\\[g(D,A)=H(D)-H(D|A)\\]

#### 信息增益比
　　以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，此时可以选择使用信息增益比（information gain ratio）  
　　特征A对训练数据集D和信息增益比\\(g_{R}(D,A)\\)定义为其信息增益g(D,A)与训练数据集D关于特征A的值的熵\\(H_{A}(D)\\)之比，即  
\\[g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}\\]
其中，\\(H_{A}(D)=-\sum_{i=1}^{n}\frac{D_{i}}{D}log_{2}\frac{|D_{i}|}{|D|}\\),n是特征A取值的个数。  


## 写在最后
　　由于决策树算法涉及知识较多，因此分为两篇文章为大家介绍。如果大家有什么疑问，可以回复公共号留言，我将尽我所能为大家进行解答，另外也可以关注我的微信公众号：机器学习让数据说话。下一期将为大家介绍决策树算法（下），敬请关注！


## 参考资料
　　《统计机器学习》	李航著
